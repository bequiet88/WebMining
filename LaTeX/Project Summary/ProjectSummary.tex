% do not change these two lines (this is a hard requirement
% there is one exception: you might replace oneside by twoside in case you deliver 
% the printed version in the accordant format
\documentclass[11pt,titlepage,oneside,openany]{book}
\usepackage{times}


\usepackage{graphicx}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{ntheorem}
\usepackage{listings}           

% \usepackage{paralist}
\usepackage{tabularx}

% this packaes are useful for nice algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

%this package adds " " after commands, allows us a nicer
%formatting of texts
\usepackage{xspace}
\usepackage[colorlinks = false, pdfborder={0 0 0}]{hyperref}
\usepackage[authoryear]{natbib}

% well, when your work is concerned with definitions, proposition and so on, we suggest this
% feel free to add Corrolary, Theorem or whatever you need
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}


% its always useful to have some shortcuts (some are specific for algorithms
% if you do not like your formating you can change it here (instead of scanning through the whole text)
\renewcommand{\algorithmiccomment}[1]{\ensuremath{\rhd} \textit{#1}}
\def\MYCALL#1#2{{\small\textsc{#1}}(\textup{#2})}
\def\MYSET#1{\scshape{#1}}
\def\MYAND{\textbf{ and }}
\def\MYOR{\textbf{ or }}
\def\MYNOT{\textbf{ not }}
\def\MYTHROW{\textbf{ throw }}
\def\MYBREAK{\textbf{break }}
\def\MYEXCEPT#1{\scshape{#1}}
\def\MYTO{\textbf{ to }}
\def\MYNIL{\textsc{Nil}}
\def\MYUNKNOWN{ unknown }
% simple stuff (not all of this is used in this examples thesis
\def\INT{{\mathcal I}} % interpretation
\def\ONT{{\mathcal O}} % ontology
\def\SEM{{\mathcal S}} % alignment semantic
\def\ALI{{\mathcal A}} % alignment
\def\USE{{\mathcal U}} % set of unsatisfiable entities
\def\CON{{\mathcal C}} % conflict set
\def\DIA{\Delta} % diagnosis
% mups and mips
\def\MUP{{\mathcal M}} % ontology
\def\MIP{{\mathcal M}} % ontology
% distributed and local entities
\newcommand{\cc}[2]{\mathit{#1}\hspace{-1pt} \# \hspace{-1pt} \mathit{#2}}
\newcommand{\cx}[1]{\mathit{#1}}
% complex stuff
\def\MER#1#2#3#4{#1 \cup_{#3}^{#2} #4} % merged ontology
\def\MUPALL#1#2#3#4#5{\textit{MUPS}_{#1}\left(#2, #3, #4, #5\right)} % the set of all mups for some concept
\def\MIPALL#1#2{\textit{MIPS}_{#1}\left(#2\right)} % the set of all mips


\begin{document}

\lstset{language=HTML} 
\pagenumbering{roman}
% lets go for the title page, something like this should be okay
\begin{titlepage}
	\vspace*{2cm}
  \begin{center}
   {\Large Extraction of Goals from Premier League Match Reports using Natural Language Processing Techniques\\}
   \vspace{2cm} 
   {Student  Project\\}
   \vspace{2cm}
   {presented by\\
    Jochen H\"{u}l\ss \xspace  \&  Matthias Rabus \\
    Matriculation Number 1376749 \& 1207834 \\
   }
   \vspace{1cm} 
   {submitted to the\\
    Chair of Information Systems V\\
    Prof.\ Dr.\ Christian\ Bizer\\
    University Mannheim\\} \vspace{2cm}
   {Mai 2013}
  \end{center}
\end{titlepage} 

% no lets make some add some table of contents
\tableofcontents
\newpage

%\listofalgorithms

\listoffigures

%\listoftables

% evntuelly you might add something like this
% \listtheorems{definition}
% \listtheorems{proposition}
\newpage


% okay, start new numbering ... here is where it really starts
\pagenumbering{arabic}

\chapter{Project Summary}
\section{Application Domain and Goals}
\label{sec:goals}
In recent years our society underwent a remerkable change in terms of the ubiquituous availability of electronically-written text. The reader might think of the rise of e-ink interfaces and the subsequent shift from print media to electronical books, papers, magazines and so forth. Apart from editorial content, a massive development of informal information sources has been taken place.\\ 
On the one hand, the blogosphere puts the right of speech on an entirely new level. On the other hand, numerous possibilitites have been emerging to customers reviewing, recommending and, thus, influencing the perception of products and services in a written way. Automatically retrieving structured information from these various sources of written text is yet a challenging task \citep[p.1]{Cellier2010}. However, robust models in this domain can even help gaining a more transparent overview in emergency situations such as disaster relief by analyzing Twitter feeds.\\

Due these plentiful resasons, the domains of text mining and Information Extraction (IE) have drawn their attention to us. Both domains tied together enable the automatic identification of selected types of entities, relations, or events in free text \citep[p.545]{Grishman2005}. Our research interest and also project goal is two-fold. Firstly, we want to find and apply patterns extracting a particular event from an unknown textual source. Secondly, the robustness of the patterns found should be assessed by classifying an unlabeled source. According to \citeauthor*{Weiss2005} \citeyearpar{Weiss2005} the first task can be referred to as Information Retrievel because we provide "clues" that determine the matching documents. They also argue that the second task is considered as a Document Classification one.\\

More specifically, we want to examplify written football match reports from England's Premiere League to investigate whether the event of a goal can be mined for automatically. With the help of manually-found seeds, our hypothesis is that a model classifying a document according to the occurence of a goal with an accuracy greater than 75\% is achievable based on automatically-labeled training data.\\

The research report is comprised of a description of the obtained data set [\ref{sec:structure}], our various preprocessing steps [\ref{sec:preproc}], the applied web data mining methods [\ref{sec:webmining}] as well as an evaluation of our results [\ref{sec:eval}].

\section{Structure and size of the data set}
\label{sec:structure}

Since many NLP tools work best with English language, we would like to increase our chance of success by using football reports of the English Premier League. The BBC sports department offers match reports for every game of the currently ongoing season 2012/2013. The amount of reports is sufficient for the purpose of this project. A Premier League season consists of 38 matchdays with 10 games each. BBC provides one report per game. The BBC sports department is a good datasource because the reports are neutrally-written and have no preference for any team. Furthermore, the used language has got a higher level and should be, compared to spoken word, easier to analyse. \\

The first step is gaterhing the data from the BBC homepage. Crawling the data will be described more detailed in the following section~\ref{sec:preproc} about preprocessing. These data was the input for the following classification process. Thus we wanted to classify each sentence wheter it contained a goal or not, we had to generate a sentence-wise input first.  The result of our crawling process was one HTML file per report. Like every HTML file, our results had a nested tree structure including some Javascript and CSS, as well. A problem could be that the text itself contained HTML tags, for example references to other reports. As mentioned before we had to extract the plain text from the rest of the document. Then we had to divide each text into the sentences itself. This could be a hard as well, because a puncuation mark could be a not sufficient seperator, for example if we consider the name of the stadium "St. James Park". The extracted sentences will be a sufficient input for the further research.

\section{Preprocessing}
\label{sec:preproc}
As outlined in the previous section, the project started with gathering the data. We used RapidMiner to crawl the data for us. RapidMiner is an open source data mining tool developed by rapid-i.com. RapidMiner allows the user to define a so called process, which is a combination of certain operators. The operators are connected by defined in- and outputs and can be parameterized to perform specific actions. \\
RapidMiner provides an operator to crawl the world wide web. BBC's Premier League result page is structured as follows: It has a main site from which every game report is liked. The reports are grouped by the day of the match. The first task of the crawler is to find the links to the game reports within the results page and then download each linked page into a seperate HTML file. Then the text has to be extracted from the HTML files. Therefor we use RapidMiner as well. The RapidMiner process takes all HTML files and extracts the plain text. This is achieved with the Cut Document-operator and an XPath-query. The extracted text is then written to one file for all reports because the Write as Text-operator can not write into multiple files. \\ 

Afterwards we had to do two steps in parallel. First we had to seperate the text file that contained the whole text from all reports and second we had to manually generate patterns that described goals.
For both tasks some helpful methods were implemented in Java. To use the text for our classifier, it had to be splitted into sentences and each sentence had to be written into an seperate file. Accomplishing this task we used a build-in Java class: BreakIterator. The class provides a method that can cut a given text into sentences given a specific language, English in this case. The previous preprocessing step created some additional lines of text, for example for every new processed file, that also have been removed within this step. As mentioned before, the results of the text splitting were saved and written into seperate text files for further use, but also processed to find some sentences containing goals and some counter examples. \\

If we want to train a classifier to distinguish the sentences for us, we will first have to create examples. Thus, there are rarely two similar sentences describing a goal, you can find certain patterns if you analyze sentences using Natural Language Processing (NLP) tools. "Natural Language Processing is a theoretically motivated range of computational techniques for analyzing and representing naturally occurring texts at one or more levels of linguistic analysis for the purpose of achieving human-like language processing for a range of tasks or applications." \citep[p.1]{Liddy2001} To construct sufficient patterns, we needed two elements of Natural Language Processing: Named Entity recognition (NER) and Part-of-Speech tagging (POS). POS assigns to each word its part of speech tag. It determines if it is a verb, noun, adjective, etc \citep[p.219]{Voutilainen2005}. Whereas, NER determines if a word is an entity, for example a name, a place or a city.  
The \hyperlink{http://nlp.stanford.edu/software/CRF-NER.shtml}{Stanford NLP Group} provides a useful Java library with many build-in language processing tools, including NER and POS tagging. Using this library, we were able to print out a sentence including named entities and part-of-speech tags. We did this for four match reports to manually detect patterns. In total, we were able to extract 14 generic patterns indicating the scoring of a goal. One of the inferred patterns is displayed in Figure~\ref{fig.manualpattern}. \\
\begin{figure} [h!]
\centering
\begin{lstlisting}[frame=single]
[PERSON POS * NN IN * corner]
\end{lstlisting}
\caption{Manual Pattern for "Rooney's volley from Mata's corner."}
\label{fig.manualpattern}
\end{figure}

As the next preprocessing step, the manual patterns had to be applied on our entire corpus of football match reports in order to find more positive training data for the subsequent classification in an automated way. For this task a search engine capable of querying for n-grams consisting of POS tags, recognized named entities, free tokens, and an arbitrary number of wildcards is needed. A n-gram is a sequence containing n literals.\\
A comprehense search engine for n-grams has been developed by \citeauthor*{Sekine2010} \citeyearpar{Sekine2010}. Their published work depicts a search engine capable of querying up to 7-grams with POS tags, named entities, and free tokens. The user interface is conveniently web-based. However, their search engine is not publicly accesible and restricted to Wikipedia articles. Moreover,  their solutions lacks a support of an arbitrary number of wildcards.\\

A thorough analysis of alternatives, also taking an own implementation into consideration, lead us the way to GATE, the "Genereal Architecture for Text Extraction". \citeauthor*{Cunningham2002} \citeyearpar[p.1]{Cunningham2002} states about their tool that "[t]he GATE architecture has enabled us not only to develop a number of successful applications for various  language  processing  tasks  (such as  Information  Extraction),  but  also to  build  and  annotate  corpora  and carry out evaluations on the applications generated." A more detailed look on GATE unveiled its powerful and extensive baseline function set. By default, GATE provides a pipeline of text processing resources for common NLP tasks. For instance, a tokeniser, a sentence splitter, a POS tagger, a named entity gezetteer, and an orthomatcher discovering relations between entities and assigning new annotations to tokens \citep[p.5]{Cunningham2002}. Each processing module can be separately added to a processing pipeline and then applied on language resources such as plain text, html, pdf, etc. employing GATE's GUI.\\ 

Apart from standard components, GATE exposes interfaces to extend its processing resources to user's requirements. One of these is the Java Annotation Patterns Engine (JAPE). JAPE was also introduced by \citeauthor*{Cunningham1999} \citeyearpar{Cunningham1999}. According to them, "[a] JAPE grammar consists of a set of phases, each of which consists of a set of pattern/action rules, and which run sequentially". Sequential pattern mining \citep{Agrawal1995} incorporates the concept that sequences of elements can repeatedly occur in a data set and thus form patterns. This idea is essential for IE and makes JAPE grammer rules a viable solution to automically label unknown language resources derived from manually-inferred patterns. "Patterns can be speciﬁed by describing a speciﬁc text string, or annotations previously created by modules such as the tokeniser, [POS tagger], gazetteer, or document format analysis"  \citep[p.5]{Cunningham2002}. A sample JAPE grammer rule depicting the manual pattern of Figure~\ref{fig.manualpattern} is shown in Figure~\ref{fig.jape}.\\
\begin{figure} [h!]
\centering
\begin{lstlisting}[frame=single]
Rule: Goal3
(
  ({Person.rule == PersonFinal}|
   {Lookup.majorType == country_adj}|
   {Token.category == NNP})
  {Token.category == POS}
  ({Token.length > 1})*
  ({Token.category == NN}|{Token.category == NNS})
  {Token.category == IN} 
  ({Token.length > 1})*
  {Token.string =~ corner}
)
:goalSequence -->
  :goalSequence.Goal = {kind=Goal, rule=Goal3}
\end{lstlisting}
\caption{Jape Grammer Rule for "Rooney's volley from Mata's corner."}
\label{fig.jape}
\end{figure}

The final preprocessing step became necessary due to GATE's restricted export capabilities. GATE outputs its annotations only as XML tags. These indicate the starting and end character position of a  positively-labeled sentence within the entire text corpus. I.e., we were bound to parse this XML tree with respect to the goal entity. This purpose served the Java API for XML Processing and some routines to write each positive and a corresponding number of negative sentences into a file. After all, we faced an evenly-distributed set of automatically-labeled sentences.

\section{Actual Web Mining}
\label{sec:webmining}
After generating the input for our model we had to find a good model for our preprocessed data. 

Matze. Verbindung zur Vorlesung. Web mining techniken (classfication: svm, naive bayes, knn). Rapidminer prozess: tokenize, weigh by info gain, validation, apply model

\section{Results and Evaluation}
\label{sec:eval}
Jochen. ZDF zu Test Set. Resultate von Precision und Recall der Classification. Diskussion. Positive und negative Einfluesse auf ergebnisse. Analog fuer ungelabelte Daten 
\section{Conclusion}
Wrap-up.

\bibliographystyle{apalike}
\bibliography{bibliography}
\end{document}
